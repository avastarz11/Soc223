---
title: "MD 08-09"
author: "Ava Foster"
format: html
embed-resources: true
editor: visual
---

# Chapter 8

We are going to practice resampling again, but this time armed with the language we learned in chapter 8. We are going to use three different datasets along the way. Don't forget to load the packages you will need.

To make life easier on yourself, be sure to specify a random number seed using `set.seed()` at the beginning of *each* of your code blocks.

```{r}
library(tidyverse)
library(here)
library(moderndive)
library(infer)
```

## Polling

### Question 1

Let's start by redoing the polling analysis we did in class. Be sure to use this exact code (including this specific `set.seed()`) to make the dataset.

```{r}
set.seed(1108)

poll <- tibble(
  vote_gop = rbinom(n = 1000,
                    size = 1,
                    prob = .53))
```

I want you to make a **bootstrap 95% percent confidence interval** for GOP (Republican) vote share based on this poll. I want you to do this two different ways to connect what you learned in chapter 7 and chapter 8.

##### First way

Calculate it using *only* the following functions:

-   `rep_sample_n()`

-   `group_by()`

-   `summarize()`

-   `quantile()`

You may also use the `pull()` or `select()` function if you need them.

A few hints:

1.  Make sure you pay attention to the `replace` option in `rep_sample_n()`.

2.  When you have the bootstrap distribution, you can get the 95 percent confidence interval by using `quantile(your_data$column, c(.025, .975))`.

```{r}
set.seed(1108)

republican_bootstrap <- poll %>%
  rep_sample_n(size = 1000, reps = 1000, replace = TRUE) %>%
  group_by(replicate) %>%
  summarize(republican = sum(vote_gop == "1") / 1000)
republican_bootstrap
```

```{r}
set.seed(1108)

quantile(republican_bootstrap$republican, c(.025, .975))
```

What is the estimated confidence interval?

-   we are 95% "confident" that the republican vote share is somewhere between 48.3975% and 54.8000%.

#### Second way

Now do it using *only* the following functions:

-   `specify()`

-   `generate()`

-   `calculate()`

-   `get_ci()`

```{r}
set.seed(1108)

republican_bootstrap_2 <- poll %>%
  specify(formula = vote_gop ~ NULL) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "mean")
republican_bootstrap_2
```

```{r}
set.seed(1108)

visualize(republican_bootstrap_2)
```

```{r}
set.seed(1108)

republican_bootstrap_2 %>%
  get_ci(level = 0.95, type = "percentile")
```

What is the estimated confidence interval? (HINT: even though you are estimating a proportion, use `stat = "mean"` instead of `stat = "prop"`.)

-   we are 95% "confident" that the republican vote share is somewhere between 48.3975% and 54.8%

How similar are the two confidence intervals you calculated? Why aren't they *exactly* the same (if you don't set the same seed)?

-   The two confidence intervals I calculated are exactly the same because I used the same seed.

## History of Rap

Here, we are going to use a dataset that contains the results of a survey conducted where artists and critics listed their favorite rap tracks. For each track we have information about its release data, and what we are going to do is to examine what is the year of that shows up the most - i.e. we are going to try to find out the year when rap peaked.

This is not a silly attempt to make a data analysis course more interesting or relatable, this is a real issue in the sociology of culture. We have evidence [that music sales are higher for old music](https://www.theatlantic.com/ideas/archive/2022/01/old-music-killing-new-music/621339/). Nostalgia is the name of the game. A lot of people really believe that old music was better and organizing their music-listening habits around this idea. So understanding where supposed "experts" place the peak of rap is important and interesting.

The data can be found [here](https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-04-14/polls.csv). Load the data and call the object `rap_poll` and then we'll get started.

```{r}
set.seed(1108)

rap_polll <- read.csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-04-14/polls.csv")
```

First of all, let's just keep the #1 song listed by each critic to make things simple. Use `filter()` to keep only those songs where `rank` is equal to 1. The resulting data frame should have 107 rows.

```{r}
rap_poll <- rap_polll %>%
  filter(rank == "1")
```

### Question 2

Make a histogram of the year each of the tracks was released.

```{r}
ggplot(rap_poll, aes(x = year)) +
  geom_histogram(binwidth = 1, color = "white", boundary = 1985) +
  labs(x = "Track Release Year")
```

What is the year of the most commonly named favorite track in this critic poll? How many critics named a track from this year?

-   The year of the most commonly named favorite track in this critic poll is 1994. Fourteen critics named a track from this year.

### Question 3

Let's assume that these 107 critics are a random sample of a population of critics. If so, it might be useful to construct a 95% confidence interval for the peak of rap. Calculate this interval, using either implementation of the percentile method I asked you to use above.

```{r}
set.seed(1108)

percentile_bootstrap <- rap_poll %>%
  rep_sample_n(size = 107, reps = 1000, replace = TRUE) %>%
  group_by(replicate) %>%
  summarize(mean_year = mean(year))
percentile_bootstrap
```

```{r}
set.seed(1108)

percentile_bootstrap %>%
  get_confidence_interval(type = "percentile", level = 0.95)
```

Report the upper and lower bound of this interval to the nearest year.

-   The upper bound is 1996 while the lower bound is 1993 for this confidence interval.

### Question 4

Let's say that instead of asking 107 critics, we had asked 25 critics. What would the confidence interval (again, rounded to the nearest year) be in that case? (HINT: you will probably need to use the "first way" for calculating a confidence interval you used above.)

```{r}
set.seed(1108)

twenty_five <- rap_poll %>%
  rep_sample_n(size = 25, reps = 1000, replace = TRUE) %>%
  group_by(replicate) %>%
  summarize(mean_year = mean(year))
twenty_five
```

```{r}
set.seed(1108)

twenty_five %>%
  get_confidence_interval(level = 0.95, type = "percentile")
```

How does the width of this confidence interval compare to the width of the confidence interval when we use the full sample of 107?

-   For the first confidence interval, the lower ci was 1993 and the upper ci was 1996. for the second confidence interval, the lower ci was 1992 and the upper ci was 1998. The width of the second confidence interval is greater than the width of the first confidence interval.

# Chapter 9

Here, we will reinforce some ideas from Chapter 9. This one is very important, not only because the content itself is important, but because you will see these ideas used and abused everywhere to support all kinds of silly arguments. A thorough understanding of these ideas will be a great BS detector when you are confronted with other people's data analyses.

We will be using soccer data from the English Premier League to examine whether home field advantage is a thing in this league. Do teams who play at home - with their own fans - win more on average? We will use your new hypothesis testing skills to examine this question. We will use data from all games of the legendary 2015/2016 season. Don't know why it's legendary? Look it up. It's worth it. Best sports story of all time some say.

Let's begin by reading in the data and looking at it.

```{r}
set.seed(1108)

pl_data <- read_csv("https://raw.githubusercontent.com/vaiseys/dav-course/main/Data/premier_league.csv")

glimpse(pl_data)
```

We have 4 columns then: the home team, the away team, the score difference (from the standpoint of the home team), and the result. We have three different types of results: an away win (aw), a home win (hw), and a draw (d). What we are interested in then is the proportion of home wins. What is the rate of winning at home and is it different that mere chance?

### Question 5

Use your data wrangling skills to calculate the proportion of home wins during the 2015/2016 season.

```{r}
set.seed(1108)

virtual_win <- pl_data %>%
  mutate(is_win = (result == "hw")) %>%
  summarize(num_win = sum(is_win)) %>%
  mutate(prop_hw = num_win / 380)
virtual_win
```

### Question 6

Now, we are going to shuffle. We are going to examine what proportion we would expect if a home win was equally likely as any other result. This one is a bit more interesting than the examples in the chapter because there are 3 choices instead of 2. Below, you will find some code that simulates our dataset, reshuffled, 1000 times. Here, however, we assume that an away-win, a draw, and a home-win have equal probability. Run the script and plot the resulting proportions as a histogram. Hint: `sampled_proportions` will be stored as a *value* in your environment. You may have to convert it to a data frame to plot as a histogram.

```{r}
set.seed(22)

sampled_proportions <- c()

for (i in 1:1000) {
  
  sampled_results <- sample(c("aw", "hw" , "d"), 
                            size = 380,
                            replace = TRUE, 
                            prob = c(1/3,1/3,1/3))
  prop <- sum(sampled_results == "hw")/380
  sampled_proportions[i] <- prop
  
}
```

```{r}
sampled_proportions <- as.data.frame(sampled_proportions)
```

```{r}
ggplot(sampled_proportions, aes(x = sampled_proportions)) +
  geom_histogram(boundary = 0.4, color = "white") +
  labs(x = "Resulting Proportions")
```

Describe the histogram in a sentence or two. How does the proportion you found in Question 5 compare to the proportion we would expect if a home win was as equally likely as any other result?

-   The histogram displays a normal distribution and is roughly bell-shaped.

### Question 7

In this scenario, what would be the null hypothesis and the alternative hypothesis? Provide enough detail so that I know you understand.

-   null hypothesis= a home win, an away win, and a draw are all equally likely outcomes.

-   alternative hypothesis= a home win outcome is more likely than an away win or a draw.

### Question 8

What would a p-value mean in this example?

-   the p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic *assuming the null hypothesis* is true. The p-value quantifies the probability and in our case it quantifies what proportion had the more "extreme" result.
